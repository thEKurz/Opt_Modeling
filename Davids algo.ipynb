{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d326c14",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) May 03 03:39:23 PM: Encountered unexpected exception importing solver ECOS:\n",
      "ImportError('numpy.core.multiarray failed to import')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) May 03 03:39:23 PM: Encountered unexpected exception importing solver ECOS_BB:\n",
      "ImportError('numpy.core.multiarray failed to import')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas_datareader import data, wb\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, integrate\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict, defaultdict\n",
    "\n",
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d00120f-d08a-4987-a13c-125fc1046921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754e689",
   "metadata": {},
   "source": [
    "## Get Universe - Current S&P 500 stocks that exhisted in 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2137c4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  504 of 504 completed\n",
      "\n",
      "3 Failed downloads:\n",
      "- BF.B: No data found for this date range, symbol may be delisted\n",
      "- CEG: Data doesn't exist for startDate = 946702800, endDate = 1641013200\n",
      "- BRK.B: No data found, symbol may be delisted\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "tickers = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    ticker = row.findAll('td')[0].text\n",
    "    tickers.append(ticker)\n",
    "\n",
    "tickers = [s.replace('\\n', '') for s in tickers]\n",
    "start = datetime(2000,1,1)\n",
    "end = datetime(2022,1,1)\n",
    "data = yf.download(tickers, start=start, end=end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c01d086",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SP_Close = data['Adj Close']\n",
    "SP_Close=SP_Close.dropna(axis=0, how='all')\n",
    "SP_Close=SP_Close.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882ab58",
   "metadata": {},
   "source": [
    "## Testing Initial Portfolio without Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2020bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_data=SP_Close[:1265] #5 years of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d2ec8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected annual return: 44.8%\n",
      "Annual volatility: 13.6%\n",
      "Sharpe Ratio: 3.15\n",
      "(0.44813890836496145, 0.13591560089320873, 3.150035062577972)\n"
     ]
    }
   ],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "\n",
    "mu = mean_historical_return(five_year_data)\n",
    "S = CovarianceShrinkage(five_year_data).ledoit_wolf()\n",
    "ef = EfficientFrontier(mu, S)\n",
    "weights = ef.max_sharpe()\n",
    "cleaned_weights = ef.clean_weights()\n",
    "print(ef.portfolio_performance(verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd1ad6c",
   "metadata": {},
   "source": [
    "## Look at some correlations and clustering to gather ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92d57956",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = five_year_data.corr()\n",
    "#plt.figure(figsize=(12,8))\n",
    "#sns.clustermap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "20952a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "#dimensions = SP_Close.shape[1]\n",
    "#plt.figure(figsize=(10, 7))  \n",
    "#plt.title(\"Dendrograms\")  \n",
    "#dend = shc.dendrogram(shc.linkage(corr, method='ward'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3ab771-5951-4ad4-9c3a-1a31063c4169",
   "metadata": {},
   "source": [
    "## Clustering algorithm using rule: select minimum number of clusters such that at most stock_pct_thresh of stocks are in clusters with an internal corr of corr_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2c4e6e32-57e4-43de-9aa6-99bafb953db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#rule is to select number of clusters clusters such that at most stock_pct_thresh of stocks are in clusters \n",
    "#with an internal corr of corr_thresh\n",
    "\n",
    "def clusters(corr, cluster_number = 45, Thresh = None):\n",
    "    if Thresh == None:\n",
    "        cluster = AgglomerativeClustering(n_clusters=cluster_number, affinity='euclidean', linkage='ward')\n",
    "    else:\n",
    "        cluster = AgglomerativeClustering(n_clusters=None, affinity='euclidean', linkage='ward',distance_threshold=Thresh) \n",
    "    kclusters = cluster.fit_predict(corr)\n",
    "    stock_clusters=pd.DataFrame(SP_Close.columns.values)\n",
    "    stock_clusters=stock_clusters.set_index(0)\n",
    "    stock_clusters['cluster']=kclusters\n",
    "    return stock_clusters\n",
    "\n",
    "def constrained_clusters(corr, corr_thresh = .7, stock_pct_thresh = .1):\n",
    "    ix = -1 #start at highest threshold and move backward\n",
    "    threshes = np.exp(np.arange(0,5,.05))#need a better system for this to make it not dependant on number of assets, \n",
    "\n",
    "    for i in range(len(threshes)):\n",
    "        thresh = threshes[ix]\n",
    "        stock_clusters = clusters(corr, Thresh = thresh)\n",
    "        cluster_number = stock_clusters['cluster'].max()+1\n",
    "\n",
    "        internal_corr_average = np.zeros([cluster_number])\n",
    "        for n in range(cluster_number):\n",
    "            cluster_corr = corr.loc[stock_clusters.loc[stock_clusters['cluster']==n].index,stock_clusters.loc[stock_clusters['cluster']==n].index]\n",
    "            internal_corr_average[n] = cluster_corr.mean().mean()\n",
    "\n",
    "        clustersWInternalCorrLessThanThreshold = np.argwhere(internal_corr_average<corr_thresh).flatten()\n",
    "        percent_stocks_in_bad_clusters = len(stock_clusters.loc[stock_clusters['cluster'].isin(clustersWInternalCorrLessThanThreshold)])/len(stock_clusters)\n",
    "        if percent_stocks_in_bad_clusters > stock_pct_thresh:\n",
    "            ix-=1\n",
    "        else:\n",
    "            break\n",
    "    print(\"# of clusters:\", cluster_number)\n",
    "    print(\"percent of stocks in clusters with internal corr of less than corr_thresh:\", percent_stocks_in_bad_clusters)\n",
    "\n",
    "    #run clustering with chosen optimal method\n",
    "    return clusters(corr, cluster_number = cluster_number)\n",
    "\n",
    "\n",
    "#test cluster corr\n",
    "\n",
    "#temp = plt.hist(internal_corr_average,bins=15)\n",
    "#plt.title(\"hist of ave of internal correlation matrices\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d8a3ba4-3aa8-48fc-98bf-cd7e1e66f683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of clusters: 45\n",
      "percent of stocks in clusters with internal corr of less than corr_thresh: 0.08174386920980926\n"
     ]
    }
   ],
   "source": [
    "clusters = constrained_clusters(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8922ab9a-e02d-4f3b-adbe-34ad991026a5",
   "metadata": {},
   "source": [
    "## backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d0591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_posterior_mean(like_pop, prior_pop):\n",
    "    mu0 = prior_pop.mean()\n",
    "    w0 = prior_pop.std()\n",
    "    dbar = like_pop.mean()\n",
    "    w = like_pop.std()\n",
    "    # Prior:\n",
    "    prior = stats.norm(mu0, w0)\n",
    "    # Likelihood:\n",
    "    like = stats.norm(dbar, w)\n",
    "    #computer posterior mean, std, distribution\n",
    "    B = w**2/(w**2+w0**2)\n",
    "    mu_s =dbar+B*(mu0-dbar)\n",
    "    w_s = w*np.sqrt(1-B)\n",
    "    posterior = stats.norm(mu_s, w_s)\n",
    "    return mu_s\n",
    "\n",
    "#calculated expected return array\n",
    "def likelihood_n_posterior_mus(n, data, period=16): #period of 16 is 3 months with weekly data\n",
    "    like_mus = np.zeros(len(data.columns))\n",
    "    post_mus = np.zeros(len(data.columns))\n",
    "    for ints,sector in enumerate(data\n",
    "                                 .columns): \n",
    "        likelihood_pop = np.array(data.iloc[n-period : n - 1][sector])\n",
    "        prior_pop = np.array(data.iloc[n-period : n - 1])\n",
    "        like_mus[ints] = likelihood_pop.mean() #normal mean opt \n",
    "        post_mus[ints] = calculate_posterior_mean(likelihood_pop,prior_pop) #bayesian means \n",
    "    return like_mus, post_mus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d85b6",
   "metadata": {},
   "source": [
    "## Markowitz Rebalancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39cbea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Markowits_Bayes_Rebalance(securities_vector, rebalance_period,prior_period,update_period=0,verbose=True,weight_bounds=(-1,1)):\n",
    "    #Bayesian update not yet implemented\n",
    "    i=1\n",
    "    i_ =round((len(securities_vector)-prior_period)/rebalance_period)\n",
    "    weights_list=[]\n",
    "    asset_return_list = []\n",
    "    dates=[]\n",
    "    \n",
    "    for p in range(0,len(securities_vector)-prior_period,rebalance_period): #\n",
    "        \n",
    "        prior_vector = securities_vector[p:prior_period+p]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Iteration \" + str(i) + \" of \" + str(i_))\n",
    "            print(str(prior_vector.index[0]) + \" - \" + str(prior_vector.index[-1]))\n",
    "            \n",
    "        mu = mean_historical_return(prior_vector)\n",
    "        S = CovarianceShrinkage(prior_vector).ledoit_wolf()\n",
    "        ef = EfficientFrontier(mu, S,weight_bounds=weight_bounds)\n",
    "        weights = ef.max_sharpe()\n",
    "        cleaned_weights = ef.clean_weights()\n",
    "        weights_list.append(cleaned_weights)\n",
    "        log_price_df = np.log(prior_vector[-rebalance_period::(max(rebalance_period-1,1))])# this is wrong should add rebalance period to both start and end index\n",
    "        returns = (log_price_df.subtract(log_price_df.iloc[1]).iloc[0]*-1).to_dict(OrderedDict)\n",
    "        asset_return_list.append(returns)\n",
    "        dates.append(prior_vector.index[-1])\n",
    "        \n",
    "        i+=1\n",
    "    weight_df=pd.DataFrame(weights_list)\n",
    "    weight_df.index=dates\n",
    "    asset_returns_df = pd.DataFrame(asset_return_list)\n",
    "    asset_returns_df.index = dates\n",
    "    return weight_df,asset_returns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "be89cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_vanilla_markowitz_weights,returns = Markowits_Bayes_Rebalance(SP_Close,21,252*5,verbose=False) #monthly rebalance with 5 year matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b2c4af9e-a9b0-4274-86f0-4f20f91d1006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2005-01-06    0.072293\n",
       "2005-02-07    0.188228\n",
       "2005-03-09    0.095174\n",
       "2005-04-08    0.079804\n",
       "2005-05-09    0.092310\n",
       "                ...   \n",
       "2021-08-13    0.946801\n",
       "2021-09-14    0.668145\n",
       "2021-10-13    0.413124\n",
       "2021-11-11    0.901721\n",
       "2021-12-13    0.767510\n",
       "Length: 204, dtype: float64"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#port returns\n",
    "np.sum(monthly_vanilla_markowitz_weights*returns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question: update clusters at each rebalance?\n",
    "#need to fix. my algorithm assumes a solution is feasible, need a better system for choosing thresholds\n",
    "#45 clusters for each rebalance\n",
    "#shrinkage works on returns markowitz works on price series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "tfenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
