{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d326c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_datareader import data, wb\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, integrate\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.risk_models import CovarianceShrinkage\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import cvxpy as cp\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9754e689",
   "metadata": {},
   "source": [
    "## Get Universe - Current S&P 500 stocks that exhisted in 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2137c4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nresp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\\nsoup = bs.BeautifulSoup(resp.text, 'lxml')\\ntable = soup.find('table', {'class': 'wikitable sortable'})\\ntickers = []\\nfor row in table.findAll('tr')[1:]:\\n    ticker = row.findAll('td')[0].text\\n    tickers.append(ticker)\\n\\ntickers = [s.replace('\\n', '') for s in tickers]\\nstart = datetime(2000,1,1)\\nend = datetime(2022,1,1)\\ndata = yf.download(tickers, start=start, end=end)\\n\\ndata.index = pd.to_datetime(data.index)\\ndata = data.sort_index()\\nSP_Close = data['Adj Close']\\nSP_Close=SP_Close.dropna(axis=0, how='all')\\nSP_Close=SP_Close.dropna(axis=1)\\npd.set_option('display.max_rows', 20 )\\nSP_Close\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "tickers = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    ticker = row.findAll('td')[0].text\n",
    "    tickers.append(ticker)\n",
    "\n",
    "tickers = [s.replace('\\n', '') for s in tickers]\n",
    "start = datetime(2000,1,1)\n",
    "end = datetime(2022,1,1)\n",
    "data = yf.download(tickers, start=start, end=end)\n",
    "\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data = data.sort_index()\n",
    "SP_Close = data['Adj Close']\n",
    "SP_Close=SP_Close.dropna(axis=0, how='all')\n",
    "SP_Close=SP_Close.dropna(axis=1)\n",
    "pd.set_option('display.max_rows', 20 )\n",
    "SP_Close\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01d086",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#read in stock data from pi\n",
    "SP_Close = pd.read_pickle('SP_Close.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882ab58",
   "metadata": {},
   "source": [
    "## Testing Initial Portfolio without Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_data=SP_Close[:1265] #5 years of data\n",
    "five_year_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ec8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = mean_historical_return(five_year_data)\n",
    "S = CovarianceShrinkage(five_year_data).ledoit_wolf()\n",
    "ef = EfficientFrontier(mu, S)\n",
    "weights = ef.max_sharpe()\n",
    "cleaned_weights = ef.clean_weights()\n",
    "#ef.save_weights_to_file(\"weights.txt\")  # saves to file\n",
    "print(ef.portfolio_performance(verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd1ad6c",
   "metadata": {},
   "source": [
    "## Look at some correlations and clustering to gather ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d57956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr = five_year_data.corr()\n",
    "#plt.figure(figsize=(12,8))\n",
    "#sns.clustermap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20952a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster.hierarchy as shc\n",
    "#plt.figure(figsize=(10, 7))  \n",
    "#plt.title(\"Dendrograms\")  \n",
    "#dend = shc.dendrogram(shc.linkage(corr, method='ward'))\n",
    "#plt.axhline(y=6, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82c5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "#cluster = AgglomerativeClustering(n_clusters=None, affinity='euclidean', linkage='ward',distance_threshold=4)  \n",
    "#kclusters = cluster.fit_predict(corr)\n",
    "#stock_clusters=pd.DataFrame(SP_Close.columns.values)\n",
    "#stock_clusters=stock_clusters.set_index(0)\n",
    "#stock_clusters['cluster']=kclusters\n",
    "#print(stock_clusters['cluster'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1f0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(stock_clusters,bins=stock_clusters['cluster'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shrinkage\n",
    "def calculate_posterior_mean(like_pop, prior_pop): #for returns use geo mean then multiply by 253 to annualize at end\n",
    "    w0 = prior_pop.std()\n",
    "    mu0 = prior_pop.mean() - w0**2/2\n",
    "    w = like_pop.std()\n",
    "    dbar = like_pop.mean() - w**2/2\n",
    "    \n",
    "    B = w**2/(w**2+w0**2)\n",
    "    mu_s =dbar+B*(mu0-dbar)\n",
    "\n",
    "    return mu_s * 253"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc4068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Portfolio_backtest(weight_vector) :\n",
    "    return_frame = SP_Close.loc[weight_vector.index]\n",
    "    return_frame = np.exp(np.log(return_frame).diff())-1\n",
    "    weighted_returns=weight_vector.shift(1)*return_frame\n",
    "    port_returns = np.sum(weighted_returns, axis=1) #shift so that we are using the weights we had over that period rather than the ones we found with hindsight\n",
    "    total_return=(port_returns+1).cumprod()\n",
    "    total_return.plot()\n",
    "    plt.title('Cumulative Return')\n",
    "    return port_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ae717-615c-46f7-be13-a3d0dbbae501",
   "metadata": {},
   "source": [
    "## Implement Clustering Influenced Dynamic Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e7c5a-0959-4983-9942-de3fc00f0116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Cluster_Constraints(clusters,u_coef,l_coef): # weight coef for how overweight you can go\n",
    "    cluster_list = set(clusters.cluster)\n",
    "    cluster_map = dict(zip(clusters.index,clusters.cluster))\n",
    "    cluster_totals=[clusters.value_counts()[x] for x in cluster_list]\n",
    "    cluster_pct=np.array(cluster_totals)/len(clusters)\n",
    "    cluster_upper_l = dict(zip(cluster_list, cluster_pct*u_coef))\n",
    "    cluster_lower_l = dict(zip(cluster_list, - l_coef*cluster_pct))\n",
    "    return cluster_map, cluster_upper_l, cluster_lower_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237eb248-0dfe-4927-823f-a57bba7f0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for testing preset clusters\n",
    "def get_clusters(corr, cluster_number = 45, Thresh = None):\n",
    "    if Thresh == None:\n",
    "        cluster = AgglomerativeClustering(n_clusters=cluster_number, affinity='euclidean', linkage='ward')\n",
    "    else:\n",
    "        cluster = AgglomerativeClustering(n_clusters=None, affinity='euclidean', linkage='ward',distance_threshold=Thresh) \n",
    "    kclusters = cluster.fit_predict(corr)\n",
    "    stock_clusters=pd.DataFrame(SP_Close.columns.values)\n",
    "    stock_clusters=stock_clusters.set_index(0)\n",
    "    stock_clusters['cluster']=kclusters\n",
    "    return stock_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daedd97-c105-49e5-ad14-b634541e1c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class constrained_clusterings_precomputed:\n",
    "    \n",
    "    def __init__(self,lookback):\n",
    "        llist = [1,2,3,4,5]\n",
    "        if (int(lookback) != lookback) or (int(lookback) not in llist):\n",
    "            raise ValueError(\"Only lookback periods of\",llist,\"supported\")\n",
    "            \n",
    "        self.df = pd.read_pickle(str(int(lookback))+'.pkl')\n",
    "        self.names_df = pd.read_pickle('names.pkl')\n",
    "        \n",
    "#         corr_threshold = 0.7\n",
    "#         pct_threshold = 0.1\n",
    "#         date = datetime.strptime('3-31-2030','%m-%d-%Y')\n",
    "        \n",
    "            \n",
    "    def calcthresholds(self,corr_threshold,pct_threshold,date,DEBUG=False):\n",
    "        if date < min(self.df.index):\n",
    "            raise ValueError(\"Date must be >=\",min(self.df.index))\n",
    "        if date > max(self.df.index):\n",
    "            print(\"WARNING: USING\",max(self.df.index),\"FOR INPUT DATE\",date)\n",
    "        inddate = max(self.df.index[self.df.index <= date])\n",
    "#         print(inddate)\n",
    "#         print(self.df.loc[inddate])\n",
    "        if DEBUG:\n",
    "            print(\"DEBUG: USING\",inddate,\"FOR INPUT DATE\",date)\n",
    "        \n",
    "        clusters = self.df.loc[inddate]['Clusters']\n",
    "        numstocks = self.df.loc[inddate]['Numstocks']\n",
    "        internals = self.df.loc[inddate]['Corrs']\n",
    "        sz = clusters.shape[0]\n",
    "        \n",
    "        tmp = np.sum((internals<corr_threshold)*(numstocks/sz),axis=1)<pct_threshold\n",
    "        ret = (np.arange(0,sz)+1)[tmp]\n",
    "        if ret.shape[0] <= 0:\n",
    "            return None\n",
    "        OPTIMAL_NUMBER_OF_CLUSTERS = ret[0]\n",
    "        if DEBUG:\n",
    "            print(\"DEBUG: OPTIMAL NUMBER OF CLUSTERS:\",OPTIMAL_NUMBER_OF_CLUSTERS)\n",
    "        \n",
    "        ret_df = self.names_df\n",
    "        ret_df = ret_df.set_index(0)\n",
    "        ret_df['cluster'] = clusters[OPTIMAL_NUMBER_OF_CLUSTERS-1,:].astype(int)\n",
    "        \n",
    "#         print(OPTIMAL_NUMBER_OF_CLUSTERS)\n",
    "#         print(ret_df)\n",
    "        return ret_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583a958-b471-44ca-b78b-b54c50fa63b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filelist = ['1.pkl','2.pkl','3.pkl','4.pkl','5.pkl','names.pkl']\n",
    "for file in filelist:\n",
    "    if not os.path.exists(file):\n",
    "        raise ValueError(\"All of\",filelist,\"must be present\")\n",
    "        \n",
    "look1 = constrained_clusterings_precomputed(1)\n",
    "look2 = constrained_clusterings_precomputed(2)\n",
    "look3 = constrained_clusterings_precomputed(3)\n",
    "look4 = constrained_clusterings_precomputed(4)\n",
    "look5 = constrained_clusterings_precomputed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cfd879-8999-4961-85c4-ccaf6bc3426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters_precomputed(lookback,corr_threshold,pct_threshold,date,DEBUG=False):\n",
    "    llist = [1,2,3,4,5]\n",
    "    if (int(lookback) != lookback) or (int(lookback) not in llist):\n",
    "        raise ValueError(\"Only lookback periods of\",llist,\"supported\")\n",
    "        \n",
    "    # Apparently pattern matching wasn't implemented until Python 3.10\n",
    "    if lookback == 1:\n",
    "        return look1.calcthresholds(corr_threshold,pct_threshold,date,DEBUG)\n",
    "    elif lookback == 2:\n",
    "        return look2.calcthresholds(corr_threshold,pct_threshold,date,DEBUG)\n",
    "    elif lookback == 3:\n",
    "        return look3.calcthresholds(corr_threshold,pct_threshold,date,DEBUG)\n",
    "    elif lookback == 4:\n",
    "        return look4.calcthresholds(corr_threshold,pct_threshold,date,DEBUG)\n",
    "    elif lookback == 5:\n",
    "        return look5.calcthresholds(corr_threshold,pct_threshold,date,DEBUG)\n",
    "    else:\n",
    "        raise ValueError(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4ea00-d4bf-43cb-82a2-72a019fb7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658d539-2392-44dc-a6aa-040bddc07832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Markowits_Bayes_Cluster_Rebalance(securities_vector, rebalance_period=21,prior_period = 253*5, update_period=0,\n",
    "                                      verbose=False,weight_bounds=(-.01,.1), cluster_u_coef=1.25, cluster_l_coef=.5,\n",
    "                                      corr_thresh = .75,leave_out_pct = .1, clust_num = None):\n",
    "    i=1\n",
    "    i_ =round((len(securities_vector)-prior_period)/rebalance_period)\n",
    "    weights_list=[]\n",
    "    dates=[]\n",
    "    cluster_list=[]\n",
    "    num=clust_num\n",
    "    for p in range(0,len(securities_vector)-prior_period,rebalance_period):\n",
    "        prior_vector = securities_vector[p:prior_period+p]\n",
    "        update_vector = securities_vector[prior_period+p-update_period:prior_period+p]\n",
    "        if verbose:\n",
    "            print(\"Iteration \" + str(i) + \" of \" + str(i_))\n",
    "            print(str(prior_vector.index[0]) + \" - \" + str(prior_vector.index[-1]))\n",
    "            \n",
    "        \n",
    "        if clust_num == None:\n",
    "            clusters= get_clusters_precomputed(prior_period/253,corr_thresh, leave_out_pct, date=prior_vector.index[-1])\n",
    "            num = clusters.max().values[0]\n",
    "            cluster_list.append(max(clusters))    \n",
    "        else:\n",
    "            clusters = get_clusters(prior_vector.corr(), cluster_number = clust_num)\n",
    "            cluster_list.append(clust_num) \n",
    "        c_map, c_upper, c_lower = Cluster_Constraints(clusters,cluster_u_coef,cluster_l_coef)\n",
    "        mu = mean_historical_return(prior_vector)  \n",
    "        if update_period != 0:\n",
    "            for i in range(num):\n",
    "                stocksNcluster = clusters.loc[clusters['cluster']==i].index\n",
    "                for stock in stocksNcluster:\n",
    "                    mu.loc[stock] = calculate_posterior_mean(update_vector[stock].pct_change().dropna().to_numpy(),\n",
    "                        prior_vector[stocksNcluster].pct_change().dropna().to_numpy())\n",
    "        \n",
    "        S = CovarianceShrinkage(prior_vector).ledoit_wolf()\n",
    "        ef = EfficientFrontier(mu, S,weight_bounds=weight_bounds)\n",
    "        ef.add_sector_constraints(c_map, c_lower, c_upper)\n",
    "        weights = ef.max_sharpe()\n",
    "        cleaned_weights = ef.clean_weights()\n",
    "        weights_list.append(cleaned_weights)\n",
    "        dates.append(prior_vector.index[-1])\n",
    "        i+=1\n",
    "    cluster_ts=pd.DataFrame(cluster_list)\n",
    "    cluster_ts.index=dates\n",
    "    cluster_ts.columns = ['Clusters']\n",
    "    weight_df=pd.DataFrame(weights_list)\n",
    "    weight_df.index=dates\n",
    "\n",
    "    return weight_df, cluster_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826be81e-5f6b-4b9b-9ae3-1921af9b48fc",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e72ea-05bd-4a49-8f67-9c5602237be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_constrained_markowitz_set_num, _ = Markowits_Bayes_Cluster_Rebalance(SP_Close,21,252*5,verbose=False,clust_num=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27def3db-4828-4fe6-8b35-fc7f21192d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Portfolio_backtest(cluster_constrained_markowitz_set_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2ca2a-28a2-4bae-9b69-056de0c8b7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import pickle\n",
    "def random_grid_search(securities_vec, number_of_samples = 50, \n",
    "                       rebal_prd_list = [63,253],#[21,63,253],\n",
    "                       prior_prd_list = [253,2*253,5*253],\n",
    "                       update_prd_list = [63, 253],\n",
    "                       low_stk_bound_list = [-.007, -.005, -.003],\n",
    "                       up_stk_bound_list = [.01,.02,.05,.1],\n",
    "                       cluster_u_coef_list = [2,3,5],\n",
    "                       cluster_l_coef_list = [-.02, -.05, -.1],\n",
    "                       corr_thesh_list = [.5,.6,.7,.8],\n",
    "                       stock_tresh_list = [.05] ):\n",
    "\n",
    "#     permutations = []\n",
    "    tuple_returns_dict = {}\n",
    "#     for rebal,prior,update,lstk,ustk,clu,cll,corr,stock in itertools.product(rebal_prd_list, prior_prd_list, update_prd_list, low_stk_bound_list, \\\n",
    "#                                                     up_stk_bound_list,cluster_u_coef_list, cluster_l_coef_list, corr_thesh_list, stock_tresh_list):\n",
    "#         if update_prd_list >= rebal_prd_list:\n",
    "#             permutations.append((rebal,prior,update,lstk,ustk,clu,cll,corr,stock))\n",
    "#     list_of_tuples = random.sample(permutations, number_of_samples)\n",
    "\n",
    "    PICKLE_NAME = 'cc3.pickle'\n",
    "    assert(\"new\" in PICKLE_NAME)\n",
    "    with open(PICKLE_NAME, 'rb') as handle:\n",
    "        list_of_tuples_all = pickle.load(handle)\n",
    "    list_of_tuples = copy.deepcopy(list_of_tuples_all)\n",
    "    \n",
    "    for tuuple_ind in range(min(number_of_samples,len(list_of_tuples))):\n",
    "        tuuple = list_of_tuples[tuuple_ind]\n",
    "        print(tuuple)\n",
    "        try:\n",
    "            weights,_ = Markowits_Bayes_Cluster_Rebalance(securities_vec, rebalance_period=tuuple[0],prior_period = tuuple[1],update_period=tuuple[2],\\\n",
    "                                      verbose=False, weight_bounds=(tuuple[3],tuuple[4]), cluster_u_coef=tuuple[5], cluster_l_coef=tuuple[6],\\\n",
    "                                          corr_thresh = tuuple[7], leave_out_pct = tuuple[8], clust_num = None)\n",
    "            performance_series = Portfolio_backtest(weights)\n",
    "            performance_series = performance_series.loc[\"2005\":]\n",
    "        except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "            break\n",
    "#             with open('res_'+str(int(time.time()))+'.pickle', 'wb') as handle:\n",
    "#                 pickle.dump(tuple_returns_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#             with open(PICKLE_NAME, 'wb') as handle:\n",
    "#                 pickle.dump(list_of_tuples_all, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#             return tuple_returns_dict\n",
    "        except:\n",
    "            weights = None\n",
    "            performance_series = None\n",
    "            print('Infeasible')\n",
    "        \n",
    "        list_of_tuples_all.pop(0)\n",
    "        tuple_returns_dict[tuuple] = performance_series\n",
    "#         print(performance_series)\n",
    "#         return performance_series\n",
    "    with open('res_'+str(int(time.time()))+'.pickle', 'wb') as handle:\n",
    "        pickle.dump(tuple_returns_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(PICKLE_NAME, 'wb') as handle:\n",
    "        pickle.dump(list_of_tuples_all, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return tuple_returns_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfdfca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "listf = [\"cc1_new.pickle\",\"cc2_new.pickle\",\"cc3_new.pickle\"]\n",
    "numf = 0\n",
    "for i in listf:\n",
    "    if os.path.exists(i):\n",
    "        numf += 1\n",
    "        namef = i\n",
    "if numf > 1:\n",
    "    print(\"Please remove all \\\"cc__new.pickle\\\" files except the one you are running:\\nDavid:\\\"cc1_new.pickle\\\"\\nTainon:\\\"cc2_new.pickle\\\"\\nEvan:\\\"cc3_new.pickle\\\"\")\n",
    "    assert(0==1)\n",
    "if numf < 1:\n",
    "    print(\"Make sure you've downloaded the right \\\"cc__new.pickle\\\" file:\\nDavid:\\\"cc1_new.pickle\\\"\\nTainon:\\\"cc2_new.pickle\\\"\\nEvan:\\\"cc3_new.pickle\\\"\")\n",
    "    assert(0==1)\n",
    "print(\"Currently running on:\\n\"+namef)\n",
    "    \n",
    "while True:\n",
    "    dictionary = random_grid_search(SP_Close[:'2018-01-01'], number_of_samples=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff8021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res_1652548211.pickle', 'rb') as handle:\n",
    "    origg = pickle.load(handle)\n",
    "print(origg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5320fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dd1.pickle', 'rb') as handle:\n",
    "    origg = pickle.load(handle)\n",
    "print(len(origg))\n",
    "for i in origg:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec43ce-926d-4171-859e-c5b424c03b23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
